{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"PCNNSRC-crossvalidation.ipynb","provenance":[{"file_id":"https://github.com/tansyab1/CNN-Sparse/blob/master/CNN-SparseCoding.ipynb","timestamp":1584549531199}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oUrdoRl9jUx6","colab_type":"code","outputId":"b314223b-25e2-4592-ea76-e57551fbd994","executionInfo":{"status":"ok","timestamp":1588082896791,"user_tz":-120,"elapsed":37092,"user":{"displayName":"SY NGUYEN TAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWpkKeV4LgC_-0TiedJJxUyazWXHzK_-ZSTdPs3A=s64","userId":"04241490624615267063"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eqHUFVQkgkd3","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.compat.v1.initializers import glorot_normal\n","import scipy.io as sio\n","import argparse\n","import random\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvaUitMIeq2a","colab_type":"code","colab":{}},"source":["class ConvAE(object):\n","    def __init__(self, n_input, kernel_size, n_hidden, reg_constant1=1.0, re_constant2=1.0, batch_size=200, train_size=100,reg=None, \\\n","                 denoise=False, model_path=None, restore_path=None, \\\n","                 logs_path='./logs'):\n","        self.n_input = n_input\n","        self.kernel_size = kernel_size\n","        self.n_hidden = n_hidden\n","        self.batch_size = batch_size\n","        self.train_size = train_size\n","        self.test_size = batch_size - train_size\n","        self.reg = reg\n","        self.model_path = model_path\n","        self.restore_path = restore_path\n","        self.iter = 0\n","        \n","        tf.compat.v1.set_random_seed(2019)\n","        weights = self._initialize_weights()\n","\n","        # input required to be fed\n","        \n","        self.train = tf.compat.v1.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])\n","        self.test = tf.compat.v1.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])\n","        self.learning_rate = tf.compat.v1.placeholder(tf.float32, [],name='learningRate')\n","\n","\n","        self.x = tf.concat([self.train, self.test], axis=0) #Concat testing and training samples\n","\n","\n","        latent, latents, shape = self.encoder(self.x, weights)\n","        latent_shape = tf.shape(input=latent)\n","\n","        # Slice the latent space features to separate training and testing latent features\n","        latent_train =  tf.slice(latent,[0,0,0,0],[self.train_size, latent_shape[1], latent_shape[2], latent_shape[3]])\n","        latent_test =  tf.slice(latent,[self.train_size,0,0,0],[self.test_size, latent_shape[1], latent_shape[2], latent_shape[3]])\n","\n","        # Vectorize the features\n","        z_train = tf.reshape(latent_train, [self.train_size, -1])\n","        z_test = tf.reshape(latent_test, [self.test_size, -1])\n","        z = tf.reshape(latent, [self.batch_size, -1])\n","\n","        Coef = weights['Coef']   # This is \\theta in the paper\n","\n","        z_test_c = tf.matmul(Coef, z_train)\n","        z_c = tf.concat([z_train, z_test_c], axis=0)\n","        latent_c_test = tf.reshape(z_test_c, tf.shape(input=latent_test)) \n","          \n","        latent_c_pretrain =  tf.concat([latent_train, latent_test], axis=0) # used in pretraining stage\n","        latent_c =  tf.concat([latent_train, latent_c_test], axis=0)        # used in the main model\n","\n","        self.x_r_pretrain = self.decoder(latent_c_pretrain, weights,  shape) # used in pretraining stage\n","        self.x_r = self.decoder(latent_c, weights,  shape)                   # used in the main model            \n","\n","\n","        self.Coef_test = Coef\n","        \n","        self.AE =  tf.concat([z_train, z_test], axis=0) # Autoencoder features to be used in benchmarks comparison\n","\n","\n","        # l_2 reconstruction loss\n","\n","        self.loss_pretrain = tf.reduce_sum(input_tensor=tf.pow(tf.subtract(self.x, self.x_r_pretrain), 2.0))\n","        \n","        self.reconst_cost_x = tf.reduce_sum(input_tensor=tf.pow(tf.subtract(self.x, self.x_r), 2.0))\n","        tf.compat.v1.summary.scalar(\"recons_loss\", self.reconst_cost_x)\n","\n","        self.reg_losses = tf.reduce_sum(input_tensor=tf.pow(Coef, 2.0))\n","        tf.compat.v1.summary.scalar(\"reg_loss\", reg_constant1 * self.reg_losses)\n","\n","        self.selfexpress_losses = 0.5 * tf.reduce_sum(input_tensor=tf.pow(tf.subtract(z_c, z), 2.0))\n","\n","        tf.compat.v1.summary.scalar(\"selfexpress_loss\", re_constant2 * self.selfexpress_losses)\n","\n","        # TOTAL LOSS\n","        self.loss = self.reconst_cost_x + reg_constant1 * self.reg_losses + 0.5 * re_constant2 * self.selfexpress_losses\n","\n","        self.merged_summary_op = tf.compat.v1.summary.merge_all()\n","        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(\n","            self.loss)  # GradientDescentOptimizer #AdamOptimizer\n","        self.optimizer_pretrain = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(\n","            self.loss_pretrain)  # GradientDescentOptimizer #AdamOptimizer\n","\n","        self.init = tf.compat.v1.global_variables_initializer()\n","        tfconfig = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n","        tfconfig.gpu_options.allow_growth = True\n","        self.sess = tf.compat.v1.InteractiveSession(config=tfconfig)\n","        self.sess.run(self.init)\n","        self.saver = tf.compat.v1.train.Saver([v for v in tf.compat.v1.trainable_variables() if not (v.name.startswith(\"Coef\"))]) # to save the pretrained model\n","        self.summary_writer = tf.compat.v1.summary.FileWriter(logs_path, graph=tf.compat.v1.get_default_graph())\n","\n","    def _initialize_weights(self):\n","        '''\n","        initializes weights for the model and soters them in a dictionary.\n","        '''\n","        \n","        all_weights = dict()\n","        all_weights['enc_w0'] = tf.compat.v1.get_variable(\"enc_w0\",\n","                                                            shape=[self.kernel_size[0], self.kernel_size[0], 1,\n","                                                                   self.n_hidden[0]],\n","                                                            initializer=glorot_normal())\n","        all_weights['enc1_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype=tf.float32))\n","\n","        all_weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype=tf.float32))\n","\n","        all_weights['enc_w1'] = tf.compat.v1.get_variable(\"enc_w1\",\n","                                                            shape=[self.kernel_size[1], self.kernel_size[1],\n","                                                                   self.n_hidden[0],\n","                                                                   self.n_hidden[1]],\n","                                                            initializer=glorot_normal())\n","        all_weights['enc_b1'] = tf.Variable(tf.zeros([self.n_hidden[1]], dtype=tf.float32))\n","\n","        all_weights['enc_w2'] = tf.compat.v1.get_variable(\"enc_w2\",\n","                                                            shape=[self.kernel_size[2], self.kernel_size[2],\n","                                                                   self.n_hidden[1],\n","                                                                   self.n_hidden[2]],\n","                                                            initializer=glorot_normal())\n","        all_weights['enc_b2'] = tf.Variable(tf.zeros([self.n_hidden[2]], dtype=tf.float32))\n","\n","        all_weights['dec_w0'] = tf.compat.v1.get_variable(\"dec1_w0\",\n","                                                            shape=[self.kernel_size[2], self.kernel_size[2],\n","                                                                   self.n_hidden[1],\n","                                                                   self.n_hidden[3]],\n","                                                            initializer=glorot_normal())\n","        all_weights['dec_b0'] = tf.Variable(tf.zeros([self.n_hidden[1]], dtype=tf.float32))\n","\n","        all_weights['dec_w1'] = tf.compat.v1.get_variable(\"dec1_w1\",\n","                                                            shape=[self.kernel_size[1], self.kernel_size[1],\n","                                                                   self.n_hidden[0],\n","                                                                   self.n_hidden[1]],\n","                                                            initializer=glorot_normal())\n","        all_weights['dec_b1'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype=tf.float32))\n","\n","        all_weights['dec_w2'] = tf.compat.v1.get_variable(\"dec1_w2\",\n","                                                            shape=[self.kernel_size[0], self.kernel_size[0], 1,\n","                                                                   self.n_hidden[0]],\n","                                                            initializer=glorot_normal())\n","        all_weights['dec_b2'] = tf.Variable(tf.zeros([1], dtype=tf.float32))\n","\n","        all_weights['enc_w3'] = tf.compat.v1.get_variable(\"enc_w3\",\n","                                                shape=[self.kernel_size[3], self.kernel_size[3],\n","                                                       self.n_hidden[2],\n","                                                       self.n_hidden[3]],\n","                                                initializer=glorot_normal())\n","        all_weights['enc_b3'] = tf.Variable(tf.zeros([self.n_hidden[3]], dtype=tf.float32))\n","\n","        all_weights['Coef'] = tf.Variable(1.0e-4 * tf.ones([self.test_size, self.train_size], tf.float32), name='Coef')\n","\n","        return all_weights\n","\n","    # Building the encoder\n","    def encoder(self, X, weights):\n","        shapes = []\n","        # Encoder Hidden layer with relu activation #1\n","        shapes.append(X.get_shape().as_list())\n","\n","        layer1 = tf.nn.bias_add(\n","            tf.nn.conv2d(input=X, filters=weights['enc_w0'], strides=[1, 2, 2, 1], padding='SAME'),\n","            weights['enc_b0'])\n","        layer1 = tf.nn.relu(layer1)\n","        layer2 = tf.nn.bias_add(\n","            tf.nn.conv2d(input=layer1, filters=weights['enc_w1'], strides=[1, 1, 1, 1], padding='SAME'),\n","            weights['enc_b1'])\n","        layer2 = tf.nn.relu(layer2)\n","        layer3 = tf.nn.bias_add(\n","            tf.nn.conv2d(input=layer2, filters=weights['enc_w2'], strides=[1, 2, 2, 1], padding='SAME'),\n","            weights['enc_b2'])\n","        layer3 = tf.nn.relu(layer3)\n","        latents = layer3\n","        # print(layer3.shape)\n"," \n","        shapes.append(layer1.get_shape().as_list())\n","        shapes.append(layer2.get_shape().as_list())\n","        layer3_in = layer3\n","\n","        latent = tf.nn.conv2d(input=layer3_in, filters=weights['enc_w3'], strides=[1, 1, 1, 1], padding='SAME')\n","        latent = tf.nn.relu(latent)\n","        shapes.append(latent.get_shape().as_list())\n","\n","        return latent, latents, shapes\n","\n","    # Building the decoder\n","    def decoder(self, z, weights, shapes):\n","        # Encoder Hidden layer with relu activation #1\n","        shape_de1 = shapes[2]\n","        layer1 = tf.add(tf.nn.conv2d_transpose(z, weights['dec_w0'], tf.stack(\n","            [tf.shape(input=self.x)[0], shape_de1[1], shape_de1[2], shape_de1[3]]), \\\n","                                               strides=[1, 2, 2, 1], padding='SAME'), weights['dec_b0'])\n","        layer1 = tf.nn.relu(layer1)\n","        shape_de2 = shapes[1]\n","        layer2 = tf.add(tf.nn.conv2d_transpose(layer1, weights['dec_w1'], tf.stack(\n","            [tf.shape(input=self.x)[0], shape_de2[1], shape_de2[2], shape_de2[3]]), \\\n","                                               strides=[1, 1, 1, 1], padding='SAME'), weights['dec_b1'])\n","        layer2 = tf.nn.relu(layer2)\n","        shape_de3 = shapes[0]\n","        layer3 = tf.add(tf.nn.conv2d_transpose(layer2, weights['dec_w2'], tf.stack(\n","            [tf.shape(input=self.x)[0], shape_de3[1], shape_de3[2], shape_de3[3]]), \\\n","                                               strides=[1, 2, 2, 1], padding='SAME'), weights['dec_b2'])\n","        layer3 = tf.nn.relu(layer3)\n","        recons = layer3\n","\n","        return recons\n","\n","    def partial_fit(self, X,Y, lr):\n","        cost, summary, _, Coef = self.sess.run(\n","            (self.reconst_cost_x, self.merged_summary_op, self.optimizer, self.Coef_test), feed_dict={self.learning_rate:lr,self.train:Y,self.test:X})\n","        self.summary_writer.add_summary(summary, self.iter)\n","        self.iter = self.iter + 1\n","        return cost, Coef\n","    \n","    def pretrain_step(self, X,Y, lr):\n","        cost, summary, _ = self.sess.run(\n","            (self.reconst_cost_x, self.merged_summary_op, self.optimizer_pretrain), feed_dict={self.learning_rate:lr,self.train:Y,self.test:X})\n","        self.summary_writer.add_summary(summary, self.iter)\n","        self.iter = self.iter + 1\n","        return cost\n","\n","    def initlization(self):\n","        self.sess.run(self.init)\n","\n","    def reconstruct(self, X):\n","        return self.sess.run(self.x_r, feed_dict={self.x:X})\n","\n","    def transform(self,  X,Y):\n","        return self.sess.run(self.AE, feed_dict={self.train:Y,self.test:X})\n","\n","    def save_model(self):\n","        save_path = self.saver.save(self.sess, self.model_path)\n","        print (\"model saved in file: %s\" % save_path)\n","\n","    def restore(self):\n","        self.saver.restore(self.sess, self.restore_path)\n","        print (\"model restored\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z20Pl-Qtgki_","colab_type":"code","colab":{}},"source":["def thrC(C, ro=0.1):\n","    if ro < 1:\n","        N1 = C.shape[0]\n","        N2 = C.shape[1]\n","        Cp = np.zeros((N1, N2))\n","        S = np.abs(np.sort(-np.abs(C), axis=0))\n","        Ind = np.argsort(-np.abs(C), axis=0)\n","        for i in range(N2):\n","            cL1 = np.sum(S[:, i]).astype(float)\n","            stop = False\n","            csum = 0\n","            t = 0\n","            while (stop == False):\n","                csum = csum + S[t, i]\n","                if csum > ro * cL1:\n","                    stop = True\n","                    Cp[Ind[0:t + 1, i], i] = C[Ind[0:t + 1, i], i]\n","                t = t + 1\n","    else:\n","        Cp = C\n","\n","    return Cp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWMmbY-FgkjJ","colab_type":"code","colab":{}},"source":["def err_rate(gt_s, s):\n","    err_x = np.sum(gt_s[:] != s[:])\n","    missrate = err_x.astype(float) / (gt_s.shape[0])\n","    return missrate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO1bBu0Xgkjn","colab_type":"code","colab":{}},"source":["def testing(CAE, num_class,args):\n","      # Split the data into training and testing sets\n","    [Img_train,Img_test,train_labels,test_labels,Label] = get_train_test_data(data,training_rate=args.rate)\n","    CAE.initlization()\n","    max_step = args.max_step  # 500 + num_class*25# 100+num_class*20\n","    pretrain_max_step = args.pretrain_step\n","    display_step = args.display_step #max_step\n","    lr = 1.0e-3\n","    \n","    epoch = 0\n","    class_ = np.zeros(np.max(test_labels))\n","    prediction = np.zeros(len(test_labels))\n","    ACC =[]\n","    Cost=[]\n","    \n","    while epoch < pretrain_max_step:\n","        [Img_train,Img_test,train_labels,test_labels,Label] = get_train_test_data(data,training_rate=args.rate)\n","\n","        Img_test = np.array(Img_test)\n","        Img_test = Img_test.astype(float)\n","        Img_train = np.array(Img_train)\n","        Img_train = Img_train.astype(float)\n","\n","        train_labels = np.array(train_labels[:])\n","        train_labels = train_labels - train_labels.min() + 1\n","        train_labels = np.squeeze(train_labels)\n","\n","        test_labels = np.array(test_labels[:])\n","        test_labels = test_labels - test_labels.min() + 1\n","        test_labels = np.squeeze(test_labels)\n","        epoch = epoch + 1\n","        cost = CAE.pretrain_step(Img_test,Img_train, lr)  #\n","\n","        if epoch % display_step == 0:\n","            print (\"pretrtain epoch: %.1d\" % epoch, \"cost: %.8f\" % (cost / float(batch_size)))   \n","    \n","    while epoch < max_step:\n","        [Im_train,Im_test,train_labels,test_labels,Label] = get_train_test_data(data,training_rate=args.rate)\n","\n","        Img_test = np.array(Img_test)\n","        Img_test = Img_test.astype(float)\n","        Img_train = np.array(Img_train)\n","        Img_train = Img_train.astype(float)\n","\n","        train_labels = np.array(train_labels[:])\n","        train_labels = train_labels - train_labels.min() + 1\n","        train_labels = np.squeeze(train_labels)\n","\n","        test_labels = np.array(test_labels[:])\n","        test_labels = test_labels - test_labels.min() + 1\n","        test_labels = np.squeeze(test_labels)\n","        epoch = epoch + 1\n","        cost, Coef = CAE.partial_fit(Img_test,Img_train, lr)  #\n","\n","                \n","        class_ = np.zeros(np.max(test_labels))\n","        prediction = np.zeros(len(test_labels))\n","        \n","        if epoch % display_step == 0:\n","            print (\"epoch: %.1d\" % epoch, \"cost: %.8f\" % (cost / float(batch_size)))   \n","            Coef = thrC(Coef)\n","            Coef= np.abs(Coef)\n","            for test_sample in range(0,len(test_labels)):\n","                x = Coef[test_sample,:]\n","                for l in range(1,np.max(test_labels)+1):\n","                    l_idx = np.array([j for j in range(0,len(train_labels)) if train_labels[j]==l])\n","                    l_idx= l_idx.astype(int)\n","                    # print(l,l_idx,np.shape(x))\n","                    class_[int(l-1)] = sum(np.abs(x[l_idx]))\n","                prediction[test_sample] = np.argmax(class_) +1\n","\n","            prediction = np.array(prediction)\n","            missrate_x = err_rate(test_labels, prediction)\n","            acc_x = 1 - missrate_x\n","            print(\"accuracy: %.4f\" % acc_x)\n","            ACC.append(acc_x)\n","            Cost.append(cost / float(batch_size))\n","    if False: # change to ture to save values in a mat file\n","        sio.savemat('./coef.mat', dict(ACC=ACC,Coef=Coef,Cost=Cost))\n","\n","    return acc_x, Coef\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTFGi_VigkkL","colab_type":"code","colab":{}},"source":["def get_train_test_data(data,training_rate=0.8):\n","    '''\n","    Extracts features and labels from the dictionary \"data,\" and splits the samples\n","    into training and testing sets.\n","    \n","    Input:\n","        data: dictionary containing two keys: {feature, Label}\n","            data['features'] : vectorized features (1024 x N)\n","            data['Label']   : groundtruth labels (1 x N)\n","        rate: ratio of the # of training samples to the total # of samples\n","        \n","    Output:\n","        training and testing sets.\n","            \n","    '''\n","\n","    Label = data['Label']\n","    Label = np.squeeze(np.array(Label))\n","    training_size = int(training_rate * len(Label))\n","    # training_size = 10\n","\n","    perm = np.random.permutation(len(Label))\n","    training_idx = perm[:training_size]\n","    testing_idx = perm[training_size:]\n","\n","    train_labels = Label[training_idx]\n","    test_labels = Label[testing_idx]\n","\n","\n","    I_test = []\n","    I_train = []\n","    img = data['features']\n","    training_img = img[:,training_idx]\n","    testing_img = img[:,testing_idx]\n","\n","    for i in range(training_img.shape[1]):\n","        temp = np.reshape(training_img[:, i], [80, 60])\n","        I_train.append(temp)\n","    Img_train = np.transpose(np.array(I_train), [0, 2, 1])\n","    Img_train = np.expand_dims(Img_train[:], 3)\n","\n","    for i in range(testing_img.shape[1]):\n","        temp = np.reshape(testing_img[:, i], [80, 60])\n","        I_test.append(temp)\n","    Img_test = np.transpose(np.array(I_test), [0, 2, 1])\n","    Img_test = np.expand_dims(Img_test[:], 3)\n","\n","    return Img_train,Img_test,train_labels,test_labels,Label\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z13paEwdgkkj","colab_type":"code","outputId":"80d47356-6576-4293-d48f-3ebde7cd154f","executionInfo":{"status":"error","timestamp":1587980950117,"user_tz":-120,"elapsed":678542,"user":{"displayName":"SY NGUYEN TAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWpkKeV4LgC_-0TiedJJxUyazWXHzK_-ZSTdPs3A=s64","userId":"04241490624615267063"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["    random.seed(2019)\n","    parser = argparse.ArgumentParser(description='')\n","    parser.add_argument('--mat', dest='mat', default='ARgender_60x80_balance', help='path of the dataset')\n","    parser.add_argument('--model', dest='model', default='umd',\n","                        help='name of the model to be saved')\n","    parser.add_argument('--rate', dest='rate', type=float, default=0.9, help='Pecentage of samples ')\n","    parser.add_argument('--epoch', dest='max_step', type=int, default=20000, help='Max # training epochs')\n","    parser.add_argument('--pretrain_step', dest='pretrain_step', type=int, default=1000, help='Max # of pretraining epochs ')\n","    parser.add_argument('--display_step', dest='display_step', type=int, default=100, help='frequency of reports')\n","\n","    parser.add_argument('-f')\n","    args = parser.parse_args()\n","\n","    # load face images and labels\n","    datapath = '/content/drive/My Drive/Colab Notebooks/Internship/dataset/'+ args.mat + '.mat'\n","    data = sio.loadmat(datapath) \n","    # global Im_train, Im_test, train_labels, test_labels\n","    # Split the data into training and testing sets\n","    [Im_train,Im_test,train_labels,test_labels,Label] = get_train_test_data(data,training_rate=args.rate)\n","    \n","    # face image clustering\n","    n_input = [60, 80]\n","    #120x165: 97.44\n","    #60x80 : 98.72/ balance : 99.92\n","    #96x96 : 98.08\n","    kernel_size = [5,3,3,1]\n","    n_hidden = [8, 16, 32,32]\n","\n","    iter_loop = 0\n","    \n","    num_class = Label.max()\n","    batch_size = len(Label)\n","    # batch_size=1000\n","    training_size = len(train_labels)\n","\n","    # These regularization values work best if the features are intensity values between 0-225\n","    reg1 = 1.0  # random.uniform(1, 10)\n","    reg2 = 8.0 # random.uniform(1, 10)\n","\n","    model_path = './models/' + args.model + '.ckpt'\n","    logs_path = './logs'\n","    tf.compat.v1.reset_default_graph()\n","    tf.compat.v1.disable_eager_execution()\n","    CAE = ConvAE(n_input=n_input, n_hidden=n_hidden, reg_constant1=reg1, re_constant2=reg2, \\\n","                 kernel_size=kernel_size, batch_size=batch_size, train_size=training_size,model_path=model_path, restore_path=model_path,\n","                 logs_path=logs_path)\n","    # CAE.summary()\n","    ACC, C = testing(CAE, num_class,args)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","pretrtain epoch: 100 cost: 11532574.72000000\n","pretrtain epoch: 200 cost: 7644874.43692308\n","pretrtain epoch: 300 cost: 7295724.30769231\n","pretrtain epoch: 400 cost: 6731598.76923077\n","pretrtain epoch: 500 cost: 6629053.83384615\n","pretrtain epoch: 600 cost: 7170545.03384615\n","pretrtain epoch: 700 cost: 6833203.20000000\n","pretrtain epoch: 800 cost: 6527422.62153846\n","pretrtain epoch: 900 cost: 6287247.36000000\n","pretrtain epoch: 1000 cost: 6639002.38769231\n","epoch: 1100 cost: 651930.58461538\n","accuracy: 0.5346\n","epoch: 1200 cost: 485830.54769231\n","accuracy: 0.4538\n","epoch: 1300 cost: 411045.41538462\n","accuracy: 0.4692\n","epoch: 1400 cost: 366881.64923077\n","accuracy: 0.5038\n","epoch: 1500 cost: 335425.08307692\n","accuracy: 0.4846\n","epoch: 1600 cost: 310433.82153846\n","accuracy: 0.5115\n","epoch: 1700 cost: 288689.74769231\n","accuracy: 0.5308\n","epoch: 1800 cost: 270686.37538462\n","accuracy: 0.4885\n","epoch: 1900 cost: 253913.92000000\n","accuracy: 0.5231\n","epoch: 2000 cost: 257263.48307692\n","accuracy: 0.5154\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BYGAE7vOFWaZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}